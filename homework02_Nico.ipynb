{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Understanding MDP's"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Chess\n",
    "State space: 8x8 spaces\n",
    "\n",
    "action space: 8x8xN<br>\n",
    "King, Queen, Bishop/Rook: Linear movements in 8 directions * max. 7 steps per direction. <br>\n",
    "Knight movement: 8 possibilities. <br>\n",
    "Pawn movements: 3 * Pawn promotions: 3<br>\n",
    "N = 73 <br>\n",
    "Policy/reward: Beat the king of the opponent.\n",
    "\n",
    "## 1.2 LunarLander\n",
    "State space: S = { coordinate x, coordinate y, linear velocity in x, linear velocity in y, angular velocity, leg one, leg two} <br>\n",
    "S = {[-90, 90], [-90, 90], [-5, 5], [-5, 5], [-3.1415, 3.1415], [0,1], [0,1]}<br>\n",
    "Action space: 4 (do nothing, fire left, fire rigfht, fire main)<br>\n",
    "Policy/reward: Land the spaceship on the landing pad.\n",
    "\n",
    "## 1.3 Model Based RL\n",
    "Reward function: Expected reward for r(t+1) given the state and action. <br>\n",
    "State transition function: Probability of state s' when taking action a in state s. <br>\n",
    "Two examples: <br>\n",
    "Discuss: No, they are not generally known. The agent has to learn these functions by taking actions and observing the states and rewards."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Implementing a GridWorld"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Links:\n",
    "1. https://towardsdatascience.com/reinforcement-learning-implement-grid-world-from-scratch-c5963765ebff\n",
    "2. https://medium.com/mlearning-ai/applying-reinforcement-learning-algorithms-to-solve-gridworld-problems-29998406dd75\n",
    "3. https://notebook.community/spro/practical-pytorch/reinforce-gridworld/reinforce-gridworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# inspired by: https://github.com/MJeremy2017/reinforcement-learning-implementation/blob/master/GridWorld/gridWorld.py\n",
    "BOARD_ROWS = 3\n",
    "BOARD_COLS = 5\n",
    "# states = 3x5\n",
    "WIN_STATE = (0, 2)\n",
    "LOSE_STATE_I = (1, 0)\n",
    "LOSE_STATE_II = (1, 4)\n",
    "START = (2, 2)\n",
    "WALL = (1, 2)\n",
    "DETERMINISTIC = True\n",
    "\n",
    "class GridWorld(): \n",
    "    def __init__(self, state=START):\n",
    "        # Initialize GridWorld board\n",
    "        self.board = np.zeros([BOARD_ROWS, BOARD_COLS])\n",
    "\n",
    "        self.state = state\n",
    "        self.det = DETERMINISTIC\n",
    "        \n",
    "    def giveReward(self):\n",
    "        if self.state == WIN_STATE:\n",
    "            return 1\n",
    "        elif (self.state == LOSE_STATE_I) or (self.state == LOSE_STATE_II):\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def nextPosition(self, action):\n",
    "        \"\"\"\n",
    "        action: up, down, left, right\n",
    "        return: next position\n",
    "        \"\"\"\n",
    "        if self.det:\n",
    "            if action == 0:\n",
    "                nextState = (self.state[0] - 1, self.state[1])\n",
    "            elif action == 1:\n",
    "                nextState = (self.state[0] + 1, self.state[1])\n",
    "            elif action == 2:\n",
    "                nextState = (self.state[0], self.state[1] - 1)\n",
    "            else: \n",
    "                nextState = (self.state[0], self.state[1] + 1)\n",
    "            # if next state legal aka. if field is free\n",
    "            if (nextState[0] >= 0) and (nextState[0] <= (BOARD_ROWS -1)):  \n",
    "                if (nextState[1] >= 0) and (nextState[1] <= (BOARD_COLS -1)):\n",
    "                    if nextState != WALL:\n",
    "                        return nextState\n",
    "            return self.state\n",
    "\n",
    "    def showBoard(self):\n",
    "        \"\"\"Show the GridWorld playfield in ASCII art\"\"\"\n",
    "        self.board[self.state] = 1\n",
    "        # Wall / barrier\n",
    "        self.board[WALL] = -1\n",
    "        # Win / Lose states\n",
    "        self.board[WIN_STATE] = 2\n",
    "        self.board[LOSE_STATE_I] = -2\n",
    "        self.board[LOSE_STATE_II] = -2\n",
    "        for i in range(0, BOARD_ROWS):\n",
    "            print('-----------------')\n",
    "            out = '| '\n",
    "            for j in range(0, BOARD_COLS):\n",
    "                # Player\n",
    "                if self.board[i, j] == 1:\n",
    "                    token = '*'\n",
    "                # Wall / barrier\n",
    "                if self.board[i, j] == -1:\n",
    "                    token = 'z'\n",
    "                # Free fields\n",
    "                if self.board[i, j] == 0:\n",
    "                    token = '0'\n",
    "                # Win state\n",
    "                if self.board[i, j] == 2:\n",
    "                    token = 'W'\n",
    "                # Lose state\n",
    "                if self.board[i, j] == -2:\n",
    "                    token = 'L'\n",
    "                out += token + ' | '\n",
    "            print(out)\n",
    "        print('-----------------')\n",
    "\n",
    "GW = GridWorld()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game end, reward -1\n",
      "Turns: 7\n",
      "---------------------\n",
      "[[[ 0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.]]\n",
      "\n",
      " [[-1.  0.  0.  0.]\n",
      "  [ 0.  0. -1. -1.]\n",
      "  [-1. -1. -1.  0.]\n",
      "  [ 0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.]]]\n",
      "---------------------\n"
     ]
    }
   ],
   "source": [
    "class Agent():\n",
    "    def __init__(self):\n",
    "        self.GW = GridWorld()\n",
    "        self.end = False\n",
    "        #self.action_list = [\"up\", \"down\", \"left\", \"right\"]\n",
    "\n",
    "        self.epsilon = 1\n",
    "        self.decay = 0.999\n",
    "\n",
    "        self.returns = {}\n",
    "        for i in range(BOARD_ROWS):\n",
    "            for j in range(BOARD_COLS):\n",
    "                for k in range(4):\n",
    "                    self.returns[(i, j), k] = [0,0] \n",
    "\n",
    "        self.q_values = np.random.uniform(size=(3,5,4))\n",
    "    \n",
    "    # state, action, reward, next_state\n",
    "    def policy(self, epsilon, state):\n",
    "        if np.random.rand() > epsilon:\n",
    "          action = np.argmax(self.q_values[state])\n",
    "        else:\n",
    "          action = np.random.randint(0,4)\n",
    "        return action\n",
    "\n",
    "    def makeAct(self):\n",
    "        self.action = self.policy(self.epsilon, self.GW.state)\n",
    "\n",
    "        self.GW.state = self.GW.nextPosition(self.action)\n",
    "        self.reward = self.GW.giveReward()\n",
    "        if self.reward==1 or self.reward==-1:\n",
    "            # End game\n",
    "            self.end = True\n",
    "        #self.q_values[self.GW.state]\n",
    "    \n",
    "    def reset(self):\n",
    "        self.GW = GridWorld()\n",
    "        self.end = False\n",
    "\n",
    "    def play(self):\n",
    "        i = 0\n",
    "        # Create empty list to safe states where the agent was in this episode\n",
    "        self.state_action_list = []\n",
    "\n",
    "        # Create empty dictionary to safe MC-estimates\n",
    "        self.erg = {}\n",
    "\n",
    "        while self.end==False:\n",
    "            last_state = self.GW.state\n",
    "            self.makeAct()\n",
    "\n",
    "            self.state_action_list.append((last_state, self.action))\n",
    "            self.epsilon *= self.decay\n",
    "            #self.state_list.append(last_state, self.action, self.GW.state)\n",
    "            i += 1\n",
    "\n",
    "        # Remove duplicates from list of states\n",
    "        self.state_ation_list = list(dict.fromkeys(self.state_action_list))\n",
    "\n",
    "        for s,a in reversed(self.state_action_list):\n",
    "            self.returns[s, a][0] += self.reward\n",
    "            self.returns[s, a][1] += 1\n",
    "        \n",
    "        for s,a in self.returns:\n",
    "            try:\n",
    "                # Safe MC-estimates per k-many visits in state to result dictionary\n",
    "                self.q_values[s[0],s[1],a] = round(self.returns[s,a][0]/self.returns[s,a][1],3)\n",
    "            except:\n",
    "                # Prevent division by 0\n",
    "                self.q_values[s[0],s[1],a] = 0\n",
    "\n",
    "        # Useful for Bugfixing\n",
    "        print('Game end, reward', self.reward)\n",
    "        print('Turns:',i)\n",
    "        print(\"---------------------\")\n",
    "        print(self.q_values)\n",
    "        print(\"---------------------\")\n",
    "        self.reset()\n",
    "\n",
    "    def showValues(self):\n",
    "        \"\"\"Show MC-estimates per state aka. field in GridWorld\"\"\"\n",
    "        for i in range(0, BOARD_ROWS):\n",
    "            print('----------------------------------------------')\n",
    "            out = '| '\n",
    "            for j in range(0, BOARD_COLS):\n",
    "                out += str(self.erg[(i, j)]).ljust(6) + ' | '\n",
    "            print(out)\n",
    "        print('----------------------------------------------')\n",
    "\n",
    "ag = Agent()\n",
    "ag.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game end, reward 1\n",
      "Turns: 10\n",
      "---------------------\n",
      "Game end, reward 1\n",
      "Turns: 17\n",
      "---------------------\n",
      "Game end, reward 1\n",
      "Turns: 21\n",
      "---------------------\n",
      "Game end, reward 1\n",
      "Turns: 10\n",
      "---------------------\n",
      "Game end, reward 1\n",
      "Turns: 8\n",
      "---------------------\n",
      "Game end, reward 1\n",
      "Turns: 15\n",
      "---------------------\n",
      "Game end, reward 1\n",
      "Turns: 5\n",
      "---------------------\n",
      "Game end, reward 1\n",
      "Turns: 18\n",
      "---------------------\n",
      "Game end, reward 1\n",
      "Turns: 10\n",
      "---------------------\n",
      "Game end, reward 1\n",
      "Turns: 11\n",
      "---------------------\n",
      "{(0, 0): 0.5, (0, 1): 1.5, (0, 2): 1.7, (0, 3): 1.2, (0, 4): 0.1, (1, 0): -0.2, (1, 1): 1.1, (1, 2): 0.0, (1, 3): 0.9, (1, 4): -0.2, (2, 0): -0.2, (2, 1): 0.9, (2, 2): 12.2, (2, 3): 0.7, (2, 4): 0.0}\n"
     ]
    }
   ],
   "source": [
    "GW.showBoard()\n",
    "\n",
    "for k in range(1,10001):\n",
    "    ag.play()\n",
    "    # Show MC-estimates per 50, 200, 500, 1000, 10000 \n",
    "    if k in [50,200,500,1000,10000]:\n",
    "        print(\"k =\",k)\n",
    "        ag.showValues()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
