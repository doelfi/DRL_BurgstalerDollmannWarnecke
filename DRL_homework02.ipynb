{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KR0l3PcQ1L-K"
      },
      "source": [
        "# 2 Learning a policy via MC - Policy Iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmh6-BLI1L-M"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# inspired by: https://github.com/MJeremy2017/reinforcement-learning-implementation/blob/master/GridWorld/gridWorld.py\n",
        "BOARD_ROWS = 3\n",
        "BOARD_COLS = 5\n",
        "# states = 3x5\n",
        "WIN_STATE = (0, 2)\n",
        "LOSE_STATE_I = (1, 0)\n",
        "LOSE_STATE_II = (1, 4)\n",
        "START = (2, 2)\n",
        "WALL = (1, 2)\n",
        "DETERMINISTIC = True\n",
        "\n",
        "class GridWorld(): \n",
        "    def __init__(self, state=START):\n",
        "        # Initialize GridWorld board\n",
        "        self.board = np.zeros([BOARD_ROWS, BOARD_COLS])\n",
        "\n",
        "        self.state = state\n",
        "        self.det = DETERMINISTIC\n",
        "        \n",
        "    def giveReward(self):\n",
        "        if self.state == WIN_STATE:\n",
        "            return 1\n",
        "        elif (self.state == LOSE_STATE_I) or (self.state == LOSE_STATE_II):\n",
        "            return -1\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def nextPosition(self, action):\n",
        "        \"\"\"\n",
        "        action: up, down, left, right\n",
        "        return: next position\n",
        "        \"\"\"\n",
        "        if self.det:\n",
        "            if action == \"up\":\n",
        "                nextState = (self.state[0] - 1, self.state[1])\n",
        "            elif action == \"down\":\n",
        "                nextState = (self.state[0] + 1, self.state[1])\n",
        "            elif action == \"left\":\n",
        "                nextState = (self.state[0], self.state[1] - 1)\n",
        "            else: \n",
        "                nextState = (self.state[0], self.state[1] + 1)\n",
        "            # if next state legal aka. if field is free\n",
        "            if (nextState[0] >= 0) and (nextState[0] <= (BOARD_ROWS -1)):  \n",
        "                if (nextState[1] >= 0) and (nextState[1] <= (BOARD_COLS -1)):\n",
        "                    if nextState != WALL:\n",
        "                        return nextState\n",
        "            return self.state\n",
        "\n",
        "    def showBoard(self):\n",
        "        \"\"\"Show the GridWorld playfield in ASCII art\"\"\"\n",
        "        self.board[self.state] = 1\n",
        "        # Wall / barrier\n",
        "        self.board[1,2] = -1\n",
        "        # Win / Lose states\n",
        "        self.board[WIN_STATE] = 2\n",
        "        self.board[LOSE_STATE_I] = -2\n",
        "        self.board[LOSE_STATE_II] = -2\n",
        "        for i in range(0, BOARD_ROWS):\n",
        "            print('-----------------')\n",
        "            out = '| '\n",
        "            for j in range(0, BOARD_COLS):\n",
        "                # Player\n",
        "                if self.board[i, j] == 1:\n",
        "                    token = '*'\n",
        "                # Wall / barrier\n",
        "                if self.board[i, j] == -1:\n",
        "                    token = 'z'\n",
        "                # Free fields\n",
        "                if self.board[i, j] == 0:\n",
        "                    token = '0'\n",
        "                # Win state\n",
        "                if self.board[i, j] == 2:\n",
        "                    token = 'W'\n",
        "                # Lose state\n",
        "                if self.board[i, j] == -2:\n",
        "                    token = 'L'\n",
        "                out += token + ' | '\n",
        "            print(out)\n",
        "        print('-----------------')\n",
        "\n",
        "GW = GridWorld()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSlzf6XO1L-R"
      },
      "source": [
        "# 3 Implementing a policy\n",
        "## 3.1 Implement the basic agent & 3.2 Evaluate the policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whXXKkw41L-S",
        "outputId": "b440d994-60cb-4402-de18-5b5b94c76542"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current position (2, 2) action up\n",
            "Next state (2, 2)\n",
            "---------------------\n"
          ]
        }
      ],
      "source": [
        "class Agent():\n",
        "    def __init__(self):\n",
        "        self.GW = GridWorld()\n",
        "        self.end = False\n",
        "        # Set of actions\n",
        "        self.action_list = [\"up\", \"down\", \"left\", \"right\"]\n",
        "\n",
        "        # Add a dictionary to safe all state values for visualization later\n",
        "        self.state_values = {}\n",
        "        for i in range(BOARD_ROWS):\n",
        "            for j in range(BOARD_COLS):\n",
        "                self.state_values[(i, j)] = [0, 0]\n",
        "\n",
        "    def distanceToWin(self):\n",
        "        ydistance = self.GW.state[0] - WIN_STATE[0]\n",
        "        xdistance = self.GW.state[1] - WIN_STATE[1]\n",
        "        return xdistance, ydistance\n",
        "    \n",
        "    def chooseAct(self):\n",
        "        xd, yd = self.distanceToWin()\n",
        "        \n",
        "        if xd==0 and yd>0:\n",
        "            action = self.action_list[0]\n",
        "        elif xd==0 and yd<0:\n",
        "            action = self.action_list[1]\n",
        "        elif xd>0 and yd==0:\n",
        "            action = self.action_list[2]\n",
        "        elif xd<0 and yd==0:\n",
        "            action = self.action_list[3]\n",
        "        # If x and y > 0 choose the smaller distance\n",
        "        elif xd>yd:\n",
        "            if xd>0:\n",
        "                action = self.action_list[2]\n",
        "            if xd<0:\n",
        "                action = self.action_list[3]\n",
        "        else:\n",
        "            if yd>0:\n",
        "                action = self.action_list[0]\n",
        "            if yd<0:\n",
        "                action = self.action_list[1]\n",
        "        return action\n",
        "    \n",
        "    # Just for showcase purposes of one episode with print commands\n",
        "    def ExamplemakeAct(self):\n",
        "        if np.random.rand() > 0.3:\n",
        "            action = self.chooseAct()\n",
        "        else:\n",
        "            action = self.action_list[np.random.randint(0,4)]\n",
        "        print(\"Current position {} action {}\".format(self.GW.state, action))\n",
        "        self.GW.state = self.GW.nextPosition(action)\n",
        "        self.reward = self.GW.giveReward()\n",
        "        print(\"Next state\", self.GW.state)\n",
        "        print(\"---------------------\")\n",
        "        if self.reward==1 or self.reward==-1:\n",
        "            # Reset game/ end episode\n",
        "            self.end = True\n",
        "            print(\"Game end, reward\", self.reward)\n",
        "            print(\"---------------------\")\n",
        "\n",
        "    def policy(self): \n",
        "        if np.random.rand() > 0.1:\n",
        "          action = self.chooseAct()\n",
        "        else:\n",
        "          action = self.action_list[np.random.randint(0,4)]\n",
        "        return action\n",
        "\n",
        "    \n",
        "    def makeAct(self, action):\n",
        "        # Choose action according to distance to goal in 90% of the time,\n",
        "        # otherwise choose action random\n",
        "\n",
        "        self.GW.state = self.GW.nextPosition(action)\n",
        "        self.reward = self.GW.giveReward()\n",
        "        if self.reward==1 or self.reward==-1:\n",
        "            # End game\n",
        "            self.end = True\n",
        "        self.state_values[self.GW.state]\n",
        "    \n",
        "    def reset(self):\n",
        "        self.GW = GridWorld()\n",
        "        self.end = False\n",
        "\n",
        "    def play(self, policy):\n",
        "        i = 0\n",
        "        # Create empty list to safe states where the agent was in this episode\n",
        "        self.state_list = []\n",
        "        self.state_list.append(self.GW.state)\n",
        "        # Create empty dictionary to safe MC-estimates\n",
        "        self.erg = {}\n",
        "\n",
        "        while self.end==False:\n",
        "            action = policy\n",
        "            self.makeAct(action)\n",
        "            self.state_list.append(self.GW.state)\n",
        "            i += 1\n",
        "\n",
        "        # Remove duplicates from list of states\n",
        "        self.state_list = list(dict.fromkeys(self.state_list))\n",
        "\n",
        "        for s in reversed(self.state_list):\n",
        "            # Add reward and state to dictionary of state values\n",
        "            self.state_values[s][0] += self.reward\n",
        "            self.state_values[s][1] += 1\n",
        "        \n",
        "        for s in ag.state_values:\n",
        "            try:\n",
        "                # Safe MC-estimates per k-many visits in state to result dictionary\n",
        "                self.erg[s] = round(self.state_values[s][0]/self.state_values[s][1],3)\n",
        "            except:\n",
        "                # Prevent division by 0\n",
        "                self.erg[s] = 0\n",
        "\n",
        "        # Useful for Bugfixing\n",
        "        #print('Game end, reward', self.reward)\n",
        "        #print('Turns:',i)\n",
        "        #print(\"---------------------\")\n",
        "        self.reset()\n",
        "\n",
        "    def showValues(self):\n",
        "        \"\"\"Show MC-estimates per state aka. field in GridWorld\"\"\"\n",
        "        for i in range(0, BOARD_ROWS):\n",
        "            print('----------------------------------------------')\n",
        "            out = '| '\n",
        "            for j in range(0, BOARD_COLS):\n",
        "                out += str(self.erg[(i, j)]).ljust(6) + ' | '\n",
        "            print(out)\n",
        "        print('----------------------------------------------')\n",
        "        \n",
        "ag = Agent()\n",
        "ag.ExamplemakeAct()\n",
        "ag.play()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKd5MymK1L-V"
      },
      "source": [
        "# 4 Visualization (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywrBrink1L-W",
        "outputId": "a569aab2-4de8-425c-dbb0-fafb997c952c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------\n",
            "| 0 | 0 | W | 0 | 0 | \n",
            "-----------------\n",
            "| L | 0 | z | 0 | L | \n",
            "-----------------\n",
            "| 0 | 0 | * | 0 | 0 | \n",
            "-----------------\n",
            "k = 50\n",
            "----------------------------------------------\n",
            "| 0.762  | 0.959  | 1.0    | 0.966  | 0.784  | \n",
            "----------------------------------------------\n",
            "| -1.0   | 0.775  | 0      | 0.784  | -1.0   | \n",
            "----------------------------------------------\n",
            "| -0.826 | 0.632  | 0.636  | 0.639  | -0.897 | \n",
            "----------------------------------------------\n",
            "k = 200\n",
            "----------------------------------------------\n",
            "| 0.765  | 0.959  | 1.0    | 0.966  | 0.785  | \n",
            "----------------------------------------------\n",
            "| -1.0   | 0.775  | 0      | 0.784  | -1.0   | \n",
            "----------------------------------------------\n",
            "| -0.822 | 0.632  | 0.636  | 0.638  | -0.896 | \n",
            "----------------------------------------------\n",
            "k = 500\n",
            "----------------------------------------------\n",
            "| 0.764  | 0.959  | 1.0    | 0.966  | 0.787  | \n",
            "----------------------------------------------\n",
            "| -1.0   | 0.776  | 0      | 0.784  | -1.0   | \n",
            "----------------------------------------------\n",
            "| -0.823 | 0.631  | 0.635  | 0.637  | -0.896 | \n",
            "----------------------------------------------\n",
            "k = 1000\n",
            "----------------------------------------------\n",
            "| 0.755  | 0.957  | 1.0    | 0.967  | 0.79   | \n",
            "----------------------------------------------\n",
            "| -1.0   | 0.774  | 0      | 0.784  | -1.0   | \n",
            "----------------------------------------------\n",
            "| -0.824 | 0.629  | 0.633  | 0.636  | -0.899 | \n",
            "----------------------------------------------\n",
            "k = 10000\n",
            "----------------------------------------------\n",
            "| 0.754  | 0.958  | 1.0    | 0.968  | 0.798  | \n",
            "----------------------------------------------\n",
            "| -1.0   | 0.777  | 0      | 0.781  | -1.0   | \n",
            "----------------------------------------------\n",
            "| -0.85  | 0.631  | 0.633  | 0.635  | -0.896 | \n",
            "----------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "GW.showBoard()\n",
        "\n",
        "for k in range(1,10001):\n",
        "    ag.play()\n",
        "    # Show MC-estimates per 50, 200, 500, 1000, 10000 \n",
        "    if k in [50,200,500,1000,10000]:\n",
        "        print(\"k =\",k)\n",
        "        ag.showValues()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mt96E87D1L-W"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "id": "yDHN39Xi1L-X",
        "outputId": "76b0a54e-4bc8-43b4-854c-f6255b58e05d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-c8996e222643>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    def policy_iteration(epsilon = 0.01, agent):\u001b[0m\n\u001b[0m                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m non-default argument follows default argument\n"
          ]
        }
      ],
      "source": [
        "agent = Agent()\n",
        "\n",
        "def policy_iteration(epsilon = 0.01, agent):\n",
        "  policy = agent.policy()\n",
        "  q_table = np.random.rand(5,3,4) # Gridworld x Actions\n",
        "  returns = dict.fromkeys(agent.state_values.keys(), (0,0))\n",
        "\n",
        "  gamma = 0.999\n",
        "\n",
        "  while True: \n",
        "    agent.play(policy) \n",
        "    G = 0 \n",
        "    for t in agent.state_list.reverse(): \n",
        "      if t == 0:\n",
        "        G = agent.reward \n",
        "      else: \n",
        "        G = gamma * G + 0  # reward only at last step of episode \n",
        "      returns(t)[0] += G \n",
        "      returns(t)[1] += 1 \n",
        "\n",
        "      q_table(t[0],t[1], \"action\") = returns(t)[0] / returns(t)[1] #this is not working\n",
        "\n",
        "      # @ToDo: we don't save the action but we need this now. \n",
        "\n",
        "      # policy is now au√üerhalb des agents, should rather be inside of agent \n",
        "      # how can we let agent choose according to highest q-value instead of minimal distance? \n",
        "\n",
        "\n",
        "      \n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0fj7tR26D9jA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}