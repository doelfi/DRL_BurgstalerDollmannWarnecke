{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Understanding MDP's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Chess\n",
    "State space: 8x8 spaces including integers to indicate the pieces / pawns on the board\n",
    "\n",
    "action space: 8x8xN<br>\n",
    "King, Queen, Bishop/Rook: Linear movements in 8 directions * max. 7 steps per direction. <br>\n",
    "Knight movement: 8 possibilities. <br>\n",
    "Pawn movements: 3 * Pawn promotions: 3<br>\n",
    "N = 73 <br>\n",
    "Reward: big reward for winning the game (negative for loosing) and small rewards for being in a material advantage\n",
    "Policy: mapping the state space to the action space -> pi(8x8x73,8x8)\n",
    "\n",
    "## 1.2 LunarLander\n",
    "State space: S = { coordinate x, coordinate y, linear velocity in x, linear velocity in y, angular velocity, leg one, leg two} <br>\n",
    "S = {[-90, 90], [-90, 90], [-5, 5], [-5, 5], [-3.1415, 3.1415], [0,1], [0,1]}<br>\n",
    "Action space: 4 (do nothing, fire left, fire rigfht, fire main)<br>\n",
    "Policy/reward: Land the spaceship on the landing pad.\n",
    "in summary: reward is increased when the lander lands correctly without making too many movements.\n",
    "\n",
    "## 1.3 Model Based RL\n",
    "the reward function maps an observation and an action to a reward value that tells the agent how well it is doing.\n",
    "E.g. in the CarRacing environment the agent gets a positive reward if it reaches a new tile and a small negative reward for every frame.\n",
    "In Pong the agent gets a positive reward for scoring a point and a negative reward for losing a point.\n",
    "\n",
    "the state transition function defines the next state given a current state and action. It can be deterministic or stochastic.\n",
    "A deterministic example would be if in the GridWorld given that the action is going down the agent will end up one square below the original state\n",
    "An example for a stochastic function would be if in the GridWorld the agent chooses an action but the action is only applied with a probability of 0.8 and it will perform a random action with a probability of 0.2\n",
    "\n",
    "Discuss: No, they are not generally known. The agent has to learn these functions by taking actions and observing the states and rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Implementing a GridWorld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Links:\n",
    "1. https://towardsdatascience.com/reinforcement-learning-implement-grid-world-from-scratch-c5963765ebff\n",
    "2. https://medium.com/mlearning-ai/applying-reinforcement-learning-algorithms-to-solve-gridworld-problems-29998406dd75\n",
    "3. https://notebook.community/spro/practical-pytorch/reinforce-gridworld/reinforce-gridworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "| 0 | 0 | W | 0 | 0 | \n",
      "-----------------\n",
      "| L | 0 | z | 0 | L | \n",
      "-----------------\n",
      "| 0 | 0 | * | 0 | 0 | \n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# inspired by: https://github.com/MJeremy2017/reinforcement-learning-implementation/blob/master/GridWorld/gridWorld.py\n",
    "BOARD_ROWS = 3\n",
    "BOARD_COLS = 5\n",
    "# states = 3x5\n",
    "WIN_STATE = (0, 2)\n",
    "LOSE_STATE_I = (1, 0)\n",
    "LOSE_STATE_II = (1, 4)\n",
    "START = (2, 2)\n",
    "DETERMINISTIC = True\n",
    "\n",
    "class GridWorld(): \n",
    "    def __init__(self, state=START):\n",
    "        # Initialize GridWorld board\n",
    "        self.board = np.zeros([BOARD_ROWS, BOARD_COLS])\n",
    "\n",
    "        self.state = state\n",
    "        self.det = DETERMINISTIC\n",
    "        \n",
    "    def giveReward(self):\n",
    "        if self.state == WIN_STATE:\n",
    "            return 1\n",
    "        elif (self.state == LOSE_STATE_I) or (self.state == LOSE_STATE_II):\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def nextPosition(self, action):\n",
    "        \"\"\"\n",
    "        action: up, down, left, right\n",
    "        return: next position\n",
    "        \"\"\"\n",
    "        if self.det:\n",
    "            if action == \"up\":\n",
    "                nextState = (self.state[0] - 1, self.state[1])\n",
    "            elif action == \"down\":\n",
    "                nextState = (self.state[0] + 1, self.state[1])\n",
    "            elif action == \"left\":\n",
    "                nextState = (self.state[0], self.state[1] - 1)\n",
    "            else: \n",
    "                nextState = (self.state[0], self.state[1] + 1)\n",
    "            # if next state legal aka. if field is free\n",
    "            if (nextState[0] >= 0) and (nextState[0] <= (BOARD_ROWS -1)):  \n",
    "                if (nextState[1] >= 0) and (nextState[1] <= (BOARD_COLS -1)):\n",
    "                    if nextState != (1,2):\n",
    "                        return nextState\n",
    "            return self.state\n",
    "\n",
    "    def showBoard(self):\n",
    "        \"\"\"Show the GridWorld playfield in ASCII art\"\"\"\n",
    "        self.board[self.state] = 1\n",
    "        # Wall / barrier\n",
    "        self.board[1,2] = -1\n",
    "        # Win / Lose states\n",
    "        self.board[WIN_STATE] = 2\n",
    "        self.board[LOSE_STATE_I] = -2\n",
    "        self.board[LOSE_STATE_II] = -2\n",
    "        for i in range(0, BOARD_ROWS):\n",
    "            print('-----------------')\n",
    "            out = '| '\n",
    "            for j in range(0, BOARD_COLS):\n",
    "                # Player\n",
    "                if self.board[i, j] == 1:\n",
    "                    token = '*'\n",
    "                # Wall / barrier\n",
    "                if self.board[i, j] == -1:\n",
    "                    token = 'z'\n",
    "                # Free fields\n",
    "                if self.board[i, j] == 0:\n",
    "                    token = '0'\n",
    "                # Win state\n",
    "                if self.board[i, j] == 2:\n",
    "                    token = 'W'\n",
    "                # Lose state\n",
    "                if self.board[i, j] == -2:\n",
    "                    token = 'L'\n",
    "                out += token + ' | '\n",
    "            print(out)\n",
    "        print('-----------------')\n",
    "\n",
    "GW = GridWorld()\n",
    "GW.showBoard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current position (2, 2) action up\n",
      "Next state (2, 2)\n",
      "---------------------\n"
     ]
    }
   ],
   "source": [
    "class Agent():\n",
    "    def __init__(self):\n",
    "        self.GW = GridWorld()\n",
    "        self.end = False\n",
    "        # Set of actions\n",
    "        self.action_list = [\"up\", \"down\", \"left\", \"right\"]\n",
    "\n",
    "        # Add a dictionary to safe all state values for visualization later\n",
    "        self.state_values = {}\n",
    "        for i in range(BOARD_ROWS):\n",
    "            for j in range(BOARD_COLS):\n",
    "                self.state_values[(i, j)] = [0, 0]\n",
    "\n",
    "    def distanceToWin(self):\n",
    "        ydistance = self.GW.state[0] - WIN_STATE[0]\n",
    "        xdistance = self.GW.state[1] - WIN_STATE[1]\n",
    "        return xdistance, ydistance\n",
    "    \n",
    "    def chooseAct(self):\n",
    "        xd, yd = self.distanceToWin()\n",
    "        \n",
    "        if xd==0 and yd>0:\n",
    "            action = self.action_list[0]\n",
    "        elif xd==0 and yd<0:\n",
    "            action = self.action_list[1]\n",
    "        elif xd>0 and yd==0:\n",
    "            action = self.action_list[2]\n",
    "        elif xd<0 and yd==0:\n",
    "            action = self.action_list[3]\n",
    "        # If x and y > 0 choose the smaller distance\n",
    "        elif xd>yd:\n",
    "            if xd>0:\n",
    "                action = self.action_list[2]\n",
    "            if xd<0:\n",
    "                action = self.action_list[3]\n",
    "        else:\n",
    "            if yd>0:\n",
    "                action = self.action_list[0]\n",
    "            if yd<0:\n",
    "                action = self.action_list[1]\n",
    "        return action\n",
    "    \n",
    "    # Just for showcase purposes of one episode with print commands\n",
    "    def ExamplemakeAct(self):\n",
    "        if np.random.rand() > 0.3:\n",
    "            action = self.chooseAct()\n",
    "        else:\n",
    "            action = self.action_list[np.random.randint(0,4)]\n",
    "        print(\"Current position {} action {}\".format(self.GW.state, action))\n",
    "        self.GW.state = self.GW.nextPosition(action)\n",
    "        self.reward = self.GW.giveReward()\n",
    "        print(\"Next state\", self.GW.state)\n",
    "        print(\"---------------------\")\n",
    "        if self.reward==1 or self.reward==-1:\n",
    "            # Reset game/ end episode\n",
    "            self.end = True\n",
    "            print(\"Game end, reward\", self.reward)\n",
    "            print(\"---------------------\")\n",
    "\n",
    "    def makeAct(self):\n",
    "        # Choose action according to distance to goal in 70% of the time,\n",
    "        # otherwise choose action random\n",
    "        if np.random.rand() > 0.3:\n",
    "            action = self.chooseAct()\n",
    "        else:\n",
    "            action = self.action_list[np.random.randint(0,4)]\n",
    "        self.GW.state = self.GW.nextPosition(action)\n",
    "        self.reward = self.GW.giveReward()\n",
    "        if self.reward==1 or self.reward==-1:\n",
    "            # End game\n",
    "            self.end = True\n",
    "        self.state_values[self.GW.state]\n",
    "    \n",
    "    def reset(self):\n",
    "        self.GW = GridWorld()\n",
    "        self.end = False\n",
    "\n",
    "    def play(self):\n",
    "        i = 0\n",
    "        # Create empty list to safe states where the agent was in this episode\n",
    "        self.state_list = []\n",
    "        self.state_list.append(self.GW.state)\n",
    "        # Create empty dictionary to safe MC-estimates\n",
    "        self.erg = {}\n",
    "\n",
    "        while self.end==False:\n",
    "            self.makeAct()\n",
    "            self.state_list.append(self.GW.state)\n",
    "            i += 1\n",
    "\n",
    "        # Remove duplicates from list of states\n",
    "        self.state_list = list(dict.fromkeys(self.state_list))\n",
    "\n",
    "        for s in reversed(self.state_list):\n",
    "            # Add reward and state to dictionary of state values\n",
    "            self.state_values[s][0] += self.reward\n",
    "            self.state_values[s][1] += 1\n",
    "        \n",
    "        for s in ag.state_values:\n",
    "            try:\n",
    "                # Safe MC-estimates per k-many visits in state to result dictionary\n",
    "                self.erg[s] = round(self.state_values[s][0]/self.state_values[s][1],3)\n",
    "            except:\n",
    "                # Prevent division by 0\n",
    "                self.erg[s] = 0\n",
    "\n",
    "        # Useful for Bugfixing\n",
    "        #print('Game end, reward', self.reward)\n",
    "        #print('Turns:',i)\n",
    "        #print(\"---------------------\")\n",
    "        self.reset()\n",
    "\n",
    "    def showValues(self):\n",
    "        \"\"\"Show MC-estimates per state aka. field in GridWorld\"\"\"\n",
    "        for i in range(0, BOARD_ROWS):\n",
    "            print('----------------------------------------------')\n",
    "            out = '| '\n",
    "            for j in range(0, BOARD_COLS):\n",
    "                out += str(self.erg[(i, j)]).ljust(6) + ' | '\n",
    "            print(out)\n",
    "        print('----------------------------------------------')\n",
    "        \n",
    "ag = Agent()\n",
    "ag.ExamplemakeAct()\n",
    "ag.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 50\n",
      "----------------------------------------------\n",
      "| 1.0    | 1.0    | 1.0    | 1.0    | 1.0    | \n",
      "----------------------------------------------\n",
      "| -1.0   | 1.0    | 0      | 0.565  | -1.0   | \n",
      "----------------------------------------------\n",
      "| -1.0   | 0.793  | 0.608  | 0.385  | -1.0   | \n",
      "----------------------------------------------\n",
      "None\n",
      "k = 200\n",
      "----------------------------------------------\n",
      "| 1.0    | 1.0    | 1.0    | 1.0    | 1.0    | \n",
      "----------------------------------------------\n",
      "| -1.0   | 0.814  | 0      | 0.744  | -1.0   | \n",
      "----------------------------------------------\n",
      "| -1.0   | 0.661  | 0.602  | 0.535  | -1.0   | \n",
      "----------------------------------------------\n",
      "None\n",
      "k = 500\n",
      "----------------------------------------------\n",
      "| 0.882  | 0.991  | 1.0    | 0.979  | 0.857  | \n",
      "----------------------------------------------\n",
      "| -1.0   | 0.798  | 0      | 0.771  | -1.0   | \n",
      "----------------------------------------------\n",
      "| -0.769 | 0.632  | 0.601  | 0.561  | -0.879 | \n",
      "----------------------------------------------\n",
      "None\n",
      "k = 1000\n",
      "----------------------------------------------\n",
      "| 0.727  | 0.973  | 1.0    | 0.964  | 0.765  | \n",
      "----------------------------------------------\n",
      "| -1.0   | 0.81   | 0      | 0.773  | -1.0   | \n",
      "----------------------------------------------\n",
      "| -0.689 | 0.676  | 0.638  | 0.609  | -0.893 | \n",
      "----------------------------------------------\n",
      "None\n",
      "k = 10000\n",
      "----------------------------------------------\n",
      "| 0.803  | 0.969  | 1.0    | 0.971  | 0.79   | \n",
      "----------------------------------------------\n",
      "| -1.0   | 0.793  | 0      | 0.787  | -1.0   | \n",
      "----------------------------------------------\n",
      "| -0.84  | 0.648  | 0.641  | 0.642  | -0.815 | \n",
      "----------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for k in range(1,10001):\n",
    "    ag.play()\n",
    "    # Show MC-estimates per 50, 200, 500, 1000, 10000 \n",
    "    if k in [50,200,500,1000,10000]:\n",
    "        print(\"k =\",k)\n",
    "        print(ag.showValues())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
